/*
*
* First, set up a backend to answer the request if there's not a cache hit.
*
*/
import std;

backend apache {

    # Set a host.
    .host = "127.0.0.1";

    # Set a port. 80 is normal Web traffic.
    .port = "<%= @apache_port %>";
}
/*
*
* Next, let's prepare a feature where we can purge or prime URLs.
* We don't want just anyone to do this. So we'll limit this special
* request type to only people on the local network.
*
*/
acl purge_prime {
    "127.0.0.1";
    <% @varnish_whitelist.each do |item| -%>
    "<%= item %>";
    <% end -%>
}

/*
*
* Next, configure the "receive" subroutine.
*
*/
sub vcl_recv {

    # Use the backend we set up above to answer the request if it's not cached.
    set req.backend = apache;
    
    # Don't cache admin pages
    if (req.url ~ "^/admin/") {
        return (pass);
    }
    
    # Don't cache munin pages
    if (req.url ~ "^/munin/") {
        return (pass);
    }

    if (req.url ~ "^/server-status") {
        return (pass);
    }
    
    # First, check to see if this is a special PURGE request.
    # If it is, and the client isn't in our special access
    # control list above, send them a cheeky message.
    
    if (req.request == "PURGE") {
        if (!client.ip ~ purge_prime) {
            error 405 "No purge for you. (" + client.ip + ")";
        }
    }
    
    # Grace mode is magical. When you have a request for an object that is in
    # the cache but has expired, Grace mode will continue to serve the old
    # expired cache object for everyone except for the very first client
    # to ask for it. Once the updated response is available for that first
    # client, all of the other clients will then get the updated response.
    #
    # By setting grace to 5 minutes, we tell Varnish to continue to serve
    # an outdated object for up to 5 minutes past its expiration to cover
    # us while we fetch an updated response. Works great for slow pages.
    
    set req.grace = 5m;
    
    # Pass the request along to lookup to see if it's in the cache.
    return(lookup);
}
/*
*
* Next, let's set up the subroutine to deal with cache misses.
*
*/
sub vcl_miss {
    return(fetch);
}
/*
*
* Now, let's set up a subroutine to deal with cache hits.
*
*/
sub vcl_hit {
    
    if (req.request == "PURGE") {
            purge;
            error 200 "Purged.";
    }
    
    return(deliver);
}
/*
*
* This is the subroutine which will fetch a response from the backend.
* It's pretty fancy because this is where the basic logic for caching is set.
*
*/
sub vcl_fetch {
    
    set beresp.ttl = <%= @varnish_ttl %>;
    set beresp.http.X-Varnish-TTL = "<%= @varnish_ttl %>";
    
    # Indicate that this response is cacheable. This is important.
    set beresp.http.X-Cacheable = "YES";
    
    # Some backends *cough* Django *cough* will assign a Vary header for
    # each User-Agent which visits the site. Varnish will store a separate
    # copy of the page in the cache for each instance of the Vary header --
    # one for each User-Agent which visits the site. This is bad. So we're
    # going to strip away the Vary header.
    unset beresp.http.Vary;

    # Now pass this backend response along to the cache to be stored and served.
    return(deliver);
}
/*
*
* Finally, let's set up a subroutine which will deliver a response to the client.
*
*/
sub vcl_deliver {
    if (obj.hits > 0) {
        set resp.http.X-Varnish-Cache = "HIT";
    }
    else {
        set resp.http.X-Varnish-Cache = "MISS";
    }
    return(deliver);
}
